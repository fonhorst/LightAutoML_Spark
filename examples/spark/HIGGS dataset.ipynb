{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c234e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/LightAutoML/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from lightautoml.dataset.roles import DatetimeRole\n",
    "\n",
    "from lightautoml.spark.tasks.base import SparkTask\n",
    "from lightautoml.spark.utils import SparkDataFrame\n",
    "from lightautoml.spark.automl.presets.tabular_presets import SparkTabularAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6af30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session():\n",
    "    if os.environ.get(\"SCRIPT_ENV\", None) == \"cluster\":\n",
    "        spark_sess = SparkSession.builder.getOrCreate()\n",
    "    else:\n",
    "        spark_sess = (\n",
    "            SparkSession\n",
    "            .builder\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.jars\", \"../../jars/spark-lightautoml_2.12-0.1.jar\")\n",
    "            .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.5\")\n",
    "            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "            .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")\n",
    "            .config(\"spark.cleaner.referenceTracking\", \"true\")\n",
    "            .config(\"spark.cleaner.periodicGC.interval\", \"1min\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "            .config(\"spark.driver.memory\", \"55g\")\n",
    "            .config(\"spark.executor.memory\", \"55g\")\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "    spark_sess.sparkContext.setCheckpointDir(\"/tmp/spark_checkpoints\")\n",
    "\n",
    "    spark_sess.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "    return spark_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d8c66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/user/projects/LightAutoML/.venv/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "https://mmlspark.azureedge.net/maven added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/user/projects/LightAutoML/.venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/user/.ivy2/jars\n",
      "com.microsoft.azure#synapseml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5478a09a-ac42-4b21-b6cc-8b40cc50b00f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#synapseml_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.azure#synapseml-core_2.12;0.9.5 in central\n",
      "\tfound org.scalactic#scalactic_2.12;3.0.5 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.4 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.2 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in user-list\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.6 in user-list\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.10 in user-list\n",
      "\tfound commons-logging#commons-logging;1.2 in user-list\n",
      "\tfound commons-codec#commons-codec;1.10 in user-list\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.6 in user-list\n",
      "\tfound com.linkedin.isolation-forest#isolation-forest_3.2.0_2.12;2.0.8 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in user-list\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in user-list\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in user-list\n",
      "\tfound org.testng#testng;6.8.8 in central\n",
      "\tfound org.beanshell#bsh;2.0b4 in local-m2-cache\n",
      "\tfound com.beust#jcommander;1.27 in local-m2-cache\n",
      "\tfound com.microsoft.azure#synapseml-deep-learning_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.azure#synapseml-opencv_2.12;0.9.5 in central\n",
      "\tfound org.openpnp#opencv;3.2.0-1 in central\n",
      "\tfound com.microsoft.cntk#cntk;2.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-cognitive_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.cognitiveservices.speech#client-jar-sdk;1.14.0 in central\n",
      "\tfound com.azure#azure-storage-blob;12.14.2 in central\n",
      "\tfound com.azure#azure-core;1.22.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.5 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in user-list\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;6.2.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound io.projectreactor#reactor-core;3.4.10 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound io.netty#netty-tcnative-boringssl-static;2.0.43.Final in central\n",
      "\tfound com.azure#azure-core-http-netty;1.11.2 in central\n",
      "\tfound io.netty#netty-handler;4.1.68.Final in central\n",
      "\tfound io.netty#netty-common;4.1.68.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.68.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.68.Final in central\n",
      "\tfound io.netty#netty-handler-proxy;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-socks;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-http;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-epoll;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-kqueue;4.1.68.Final in central\n",
      "\tfound io.projectreactor.netty#reactor-netty-http;1.0.11 in central\n",
      "\tfound io.netty#netty-resolver-dns;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-dns;4.1.68.Final in central\n",
      "\tfound io.netty#netty-resolver-dns-native-macos;4.1.68.Final in central\n",
      "\tfound io.projectreactor.netty#reactor-netty-core;1.0.11 in central\n",
      "\tfound com.azure#azure-storage-common;12.14.1 in central\n",
      "\tfound com.azure#azure-storage-internal-avro;12.1.2 in central\n",
      "\tfound com.azure#azure-ai-textanalytics;5.1.4 in central\n",
      "\tfound com.microsoft.azure#synapseml-vw_2.12;0.9.5 in central\n",
      "\tfound com.github.vowpalwabbit#vw-jni;8.9.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-lightgbm_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.ml.lightgbm#lightgbmlib;3.2.110 in central\n",
      ":: resolution report :: resolve 6852ms :: artifacts dl 84ms\n",
      "\t:: modules in use:\n",
      "\tcom.azure#azure-ai-textanalytics;5.1.4 from central in [default]\n",
      "\tcom.azure#azure-core;1.22.0 from central in [default]\n",
      "\tcom.azure#azure-core-http-netty;1.11.2 from central in [default]\n",
      "\tcom.azure#azure-storage-blob;12.14.2 from central in [default]\n",
      "\tcom.azure#azure-storage-common;12.14.1 from central in [default]\n",
      "\tcom.azure#azure-storage-internal-avro;12.1.2 from central in [default]\n",
      "\tcom.beust#jcommander;1.27 from local-m2-cache in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from user-list in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;6.2.4 from central in [default]\n",
      "\tcom.github.vowpalwabbit#vw-jni;8.9.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from user-list in [default]\n",
      "\tcom.linkedin.isolation-forest#isolation-forest_3.2.0_2.12;2.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-cognitive_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-core_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-deep-learning_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-lightgbm_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-opencv_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-vw_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.cntk#cntk;2.4 from central in [default]\n",
      "\tcom.microsoft.cognitiveservices.speech#client-jar-sdk;1.14.0 from central in [default]\n",
      "\tcom.microsoft.ml.lightgbm#lightgbmlib;3.2.110 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.10 from user-list in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from user-list in [default]\n",
      "\tio.netty#netty-buffer;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-dns;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-socks;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-handler-proxy;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver-dns;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver-dns-native-macos;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-boringssl-static;2.0.43.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-epoll;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-kqueue;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.68.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.4.10 from central in [default]\n",
      "\tio.projectreactor.netty#reactor-netty-core;1.0.11 from central in [default]\n",
      "\tio.projectreactor.netty#reactor-netty-http;1.0.11 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.2 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from user-list in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.6 from user-list in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.10 from user-list in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.6 from user-list in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
      "\torg.beanshell#bsh;2.0b4 from local-m2-cache in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.openpnp#opencv;3.2.0-1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.4 from central in [default]\n",
      "\torg.scalactic#scalactic_2.12;3.0.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from user-list in [default]\n",
      "\torg.testng#testng;6.8.8 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from local-m2-cache in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from user-list in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   69  |   2   |   2   |   0   ||   69  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5478a09a-ac42-4b21-b6cc-8b40cc50b00f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 69 already retrieved (0kB/25ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/24 11:41:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/24 11:41:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9748d96",
   "metadata": {},
   "source": [
    "# Dataset reading\n",
    "\n",
    "(reads only 1M rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b807dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark. \\\n",
    "        read. \\\n",
    "        csv(\"file:///opt/spark_data/HIGGS.csv\", header=False, escape=\"\\\"\"). \\\n",
    "        limit(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0125a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.cache()\n",
    "data.write.mode('overwrite').format('noop').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e47cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434eb177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ec4b6",
   "metadata": {},
   "source": [
    "## Data Set Information:\n",
    "\n",
    "The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.\n",
    "\n",
    "## Attribute Information:\n",
    "\n",
    "The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. For more detailed information about each feature see the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f59ff",
   "metadata": {},
   "source": [
    "# Divide into train and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1e1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string, _c27: string, _c28: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed)\n",
    "train_data.write.mode('overwrite').format('noop').save()\n",
    "test_data.write.mode('overwrite').format('noop').save()\n",
    "\n",
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2a80e",
   "metadata": {},
   "source": [
    "# AutoML params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ac8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {\n",
    "    \"target\": \"_c0\"\n",
    "}\n",
    "task = SparkTask(\"binary\")\n",
    "use_algos = [[\"lgb\"]]\n",
    "cv = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c68680",
   "metadata": {},
   "source": [
    "# Fitting and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f8dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/LightAutoML/lightautoml/spark/ml_algo/boost_lgbm.py:398: RuntimeWarning: Maximum validation size for SparkBoostLGBM is exceeded: 401067 > 10000. Reducing validation size down to maximum.\n",
      "  warnings.warn(f\"Maximum validation size for SparkBoostLGBM is exceeded: {valid_size} > {max_val_size}. \"\n",
      "[Stage 214:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 211715, number of negative: 187586\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0,318274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6132\n",
      "[LightGBM] [Info] Number of data points in the train set: 399301, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,530214 -> initscore=0,121004\n",
      "[LightGBM] [Info] Start training from score 0,121004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/LightAutoML/lightautoml/spark/utils.py:191: RuntimeWarning: Attempting to calculate shape on not cached dataframe. It may take too much time.\n",
      "  warnings.warn(\"Attempting to calculate shape on not cached dataframe. \"\n",
      "/home/user/projects/LightAutoML/lightautoml/spark/ml_algo/boost_lgbm.py:398: RuntimeWarning: Maximum validation size for SparkBoostLGBM is exceeded: 401067 > 10000. Reducing validation size down to maximum.\n",
      "  warnings.warn(f\"Maximum validation size for SparkBoostLGBM is exceeded: {valid_size} > {max_val_size}. \"\n",
      "[Stage 232:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 211715, number of negative: 187586\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0,038219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6132\n",
      "[LightGBM] [Info] Number of data points in the train set: 399301, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,530214 -> initscore=0,121004\n",
      "[LightGBM] [Info] Start training from score 0,121004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/LightAutoML/lightautoml/spark/ml_algo/boost_lgbm.py:398: RuntimeWarning: Maximum validation size for SparkBoostLGBM is exceeded: 399301 > 10000. Reducing validation size down to maximum.\n",
      "  warnings.warn(f\"Maximum validation size for SparkBoostLGBM is exceeded: {valid_size} > {max_val_size}. \"\n",
      "[Stage 241:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 212539, number of negative: 188528\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0,033555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6132\n",
      "[LightGBM] [Info] Number of data points in the train set: 401067, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,529934 -> initscore=0,119879\n",
      "[LightGBM] [Info] Start training from score 0,119879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "automl = SparkTabularAutoML(\n",
    "    spark=spark,\n",
    "    task=task,\n",
    "    general_params={\"use_algos\": use_algos},\n",
    "    lgb_params={'use_single_dataset_mode': True },\n",
    "    reader_params={\"cv\": cv, \"advanced_roles\": False}\n",
    ")\n",
    "\n",
    "oof_predictions = automl.fit_predict(\n",
    "    train_data,\n",
    "    roles=roles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794eae1e",
   "metadata": {},
   "source": [
    "# Score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07cbb28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 277:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF score: 0.8247978240800539\n",
      "TEST score: 0.8291288777512692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "score = task.get_dataset_metric()\n",
    "metric_value = score(oof_predictions)\n",
    "\n",
    "te_pred = automl.predict(test_data, add_reader_attrs=True)\n",
    "score = task.get_dataset_metric()\n",
    "test_metric_value = score(te_pred)\n",
    "\n",
    "print(f\"OOF score: {metric_value}\")\n",
    "print(f\"TEST score: {test_metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db589d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39459b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88452266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade1e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef509e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0131e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ec9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a0467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f877f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefd085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
