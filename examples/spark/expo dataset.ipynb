{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c234e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/LightAutoML/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from lightautoml.dataset.roles import DatetimeRole\n",
    "\n",
    "from lightautoml.spark.tasks.base import SparkTask\n",
    "from lightautoml.spark.utils import SparkDataFrame\n",
    "from lightautoml.spark.automl.presets.tabular_presets import SparkTabularAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6af30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session():\n",
    "    if os.environ.get(\"SCRIPT_ENV\", None) == \"cluster\":\n",
    "        spark_sess = SparkSession.builder.getOrCreate()\n",
    "    else:\n",
    "        spark_sess = (\n",
    "            SparkSession\n",
    "            .builder\n",
    "            .master(\"local[*]\")\n",
    "            .config(\"spark.jars\", \"../../jars/spark-lightautoml_2.12-0.1.jar\")\n",
    "            .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.5\")\n",
    "            .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "            .config(\"spark.cleaner.referenceTracking.cleanCheckpoints\", \"true\")\n",
    "            .config(\"spark.cleaner.referenceTracking\", \"true\")\n",
    "            .config(\"spark.cleaner.periodicGC.interval\", \"1min\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "            .config(\"spark.driver.memory\", \"55g\")\n",
    "            .config(\"spark.executor.memory\", \"55g\")\n",
    "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "    spark_sess.sparkContext.setCheckpointDir(\"/tmp/spark_checkpoints\")\n",
    "\n",
    "    spark_sess.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "    return spark_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d8c66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/user/projects/LightAutoML/.venv/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "https://mmlspark.azureedge.net/maven added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/user/projects/LightAutoML/.venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/user/.ivy2/jars\n",
      "com.microsoft.azure#synapseml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2255a58a-d00b-4a02-8998-6a0f112d8434;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#synapseml_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.azure#synapseml-core_2.12;0.9.5 in central\n",
      "\tfound org.scalactic#scalactic_2.12;3.0.5 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.4 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.2 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in user-list\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.6 in user-list\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.10 in user-list\n",
      "\tfound commons-logging#commons-logging;1.2 in user-list\n",
      "\tfound commons-codec#commons-codec;1.10 in user-list\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.6 in user-list\n",
      "\tfound com.linkedin.isolation-forest#isolation-forest_3.2.0_2.12;2.0.8 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in user-list\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in user-list\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in user-list\n",
      "\tfound org.testng#testng;6.8.8 in central\n",
      "\tfound org.beanshell#bsh;2.0b4 in local-m2-cache\n",
      "\tfound com.beust#jcommander;1.27 in local-m2-cache\n",
      "\tfound com.microsoft.azure#synapseml-deep-learning_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.azure#synapseml-opencv_2.12;0.9.5 in central\n",
      "\tfound org.openpnp#opencv;3.2.0-1 in central\n",
      "\tfound com.microsoft.cntk#cntk;2.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-cognitive_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.cognitiveservices.speech#client-jar-sdk;1.14.0 in central\n",
      "\tfound com.azure#azure-storage-blob;12.14.2 in central\n",
      "\tfound com.azure#azure-core;1.22.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.5 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in user-list\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;6.2.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound io.projectreactor#reactor-core;3.4.10 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound io.netty#netty-tcnative-boringssl-static;2.0.43.Final in central\n",
      "\tfound com.azure#azure-core-http-netty;1.11.2 in central\n",
      "\tfound io.netty#netty-handler;4.1.68.Final in central\n",
      "\tfound io.netty#netty-common;4.1.68.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.68.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.68.Final in central\n",
      "\tfound io.netty#netty-handler-proxy;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-socks;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-http;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-epoll;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-kqueue;4.1.68.Final in central\n",
      "\tfound io.projectreactor.netty#reactor-netty-http;1.0.11 in central\n",
      "\tfound io.netty#netty-resolver-dns;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-dns;4.1.68.Final in central\n",
      "\tfound io.netty#netty-resolver-dns-native-macos;4.1.68.Final in central\n",
      "\tfound io.projectreactor.netty#reactor-netty-core;1.0.11 in central\n",
      "\tfound com.azure#azure-storage-common;12.14.1 in central\n",
      "\tfound com.azure#azure-storage-internal-avro;12.1.2 in central\n",
      "\tfound com.azure#azure-ai-textanalytics;5.1.4 in central\n",
      "\tfound com.microsoft.azure#synapseml-vw_2.12;0.9.5 in central\n",
      "\tfound com.github.vowpalwabbit#vw-jni;8.9.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-lightgbm_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.ml.lightgbm#lightgbmlib;3.2.110 in central\n",
      ":: resolution report :: resolve 4749ms :: artifacts dl 44ms\n",
      "\t:: modules in use:\n",
      "\tcom.azure#azure-ai-textanalytics;5.1.4 from central in [default]\n",
      "\tcom.azure#azure-core;1.22.0 from central in [default]\n",
      "\tcom.azure#azure-core-http-netty;1.11.2 from central in [default]\n",
      "\tcom.azure#azure-storage-blob;12.14.2 from central in [default]\n",
      "\tcom.azure#azure-storage-common;12.14.1 from central in [default]\n",
      "\tcom.azure#azure-storage-internal-avro;12.1.2 from central in [default]\n",
      "\tcom.beust#jcommander;1.27 from local-m2-cache in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from user-list in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;6.2.4 from central in [default]\n",
      "\tcom.github.vowpalwabbit#vw-jni;8.9.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from user-list in [default]\n",
      "\tcom.linkedin.isolation-forest#isolation-forest_3.2.0_2.12;2.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-cognitive_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-core_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-deep-learning_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-lightgbm_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-opencv_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-vw_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.cntk#cntk;2.4 from central in [default]\n",
      "\tcom.microsoft.cognitiveservices.speech#client-jar-sdk;1.14.0 from central in [default]\n",
      "\tcom.microsoft.ml.lightgbm#lightgbmlib;3.2.110 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.10 from user-list in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from user-list in [default]\n",
      "\tio.netty#netty-buffer;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-dns;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-socks;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-handler-proxy;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver-dns;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver-dns-native-macos;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-boringssl-static;2.0.43.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-epoll;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-kqueue;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.68.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.4.10 from central in [default]\n",
      "\tio.projectreactor.netty#reactor-netty-core;1.0.11 from central in [default]\n",
      "\tio.projectreactor.netty#reactor-netty-http;1.0.11 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.2 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from user-list in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.6 from user-list in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.10 from user-list in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.6 from user-list in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
      "\torg.beanshell#bsh;2.0b4 from local-m2-cache in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.openpnp#opencv;3.2.0-1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.4 from central in [default]\n",
      "\torg.scalactic#scalactic_2.12;3.0.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from user-list in [default]\n",
      "\torg.testng#testng;6.8.8 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from local-m2-cache in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from user-list in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   69  |   2   |   2   |   0   ||   69  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2255a58a-d00b-4a02-8998-6a0f112d8434\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 69 already retrieved (0kB/24ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/24 11:26:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac11db",
   "metadata": {},
   "source": [
    "# Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b807dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"file:///opt/spark_data/expo/1990.csv\", header=True, escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434eb177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- DayofMonth: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: string (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: string (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: string (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: string (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09956d30",
   "metadata": {},
   "source": [
    "\n",
    "* Year [int]: Year of the dataset (1999 & 2000)\n",
    "* Month [int]: Month of the observation (1 - Jan, 2 - Feb, etc.)\n",
    "* DayofMonth [int]: Day of the month (1 - 31, if applicable)\n",
    "* DayOfWeek [int]: Day of the week (1 - Mon, 2 - Tue, etc.)\n",
    "* DepTime [int]: Actual departure time (local time zone %H%M format)\n",
    "* CRSDepTime [int]: Scheduled departure time (local time zone %H%M format)\n",
    "* ArrTime [int]: Actual arrival time (local time zone %H%M format)\n",
    "* CRSArrTime [int]: Scheduled arrival time (local time zone %H%M format)\n",
    "* UniqueCarrier [int]: Unique carrier code to identify the carriers in carriers.csv\n",
    "* FlightNum [int]: Flight number\n",
    "* TailNum [str]: Unique tail number to identify the planes in plane-data.csv\n",
    "* ActualElapsedTime [int]: Difference between ArrTime and DepTime in minutes, also sum of AirTime, TaxiIn, TaxiOut\n",
    "* CRSElapsedTime [int]: Difference between CRSArrTime and CRSDepTime in minutes\n",
    "* AirTime [int]: Air time in minutes\n",
    "* ArrDelay [int]: Difference between ArrTime and CRSArrTime in minutes\n",
    "* DepDelay [int]: Difference between DepTime and CRSDepTime in minutes\n",
    "* Origin [str]: Unique IATA airport code that flight was departed from, can be identified in airports.csv\n",
    "* Dest [str]: Unique IATA airport code for flight destination, can be identified in airports.csv\n",
    "* Distance [int]: Flight distance in miles\n",
    "* TaxiIn [int]: Taxi-in time in minutes\n",
    "* TaxiOut [int]: Taxi-out time in minutes\n",
    "* Cancelled [int]: Flight cancellation (1 - Cancelled, 0 - Not Cancelled)\n",
    "* CancellationCode [str]: Flight cancellation reason (A - Carrier, B - Weather, C - National Aviation System, D - Security)\n",
    "* Diverted [int]: Fight diverted (1 - Diverted, 0 - Not diverted)\n",
    "* CarrierDelay [int]: Delay caused by carrier in minutes\n",
    "* WeatherDelay [int]: Delay caused by weather in minutes\n",
    "* NASDelay [int]: Delay caused by National Aviation System in minutes\n",
    "* LateAircraftDelay [int]: Delay caused by previous late flight arrivals in minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec2cc5",
   "metadata": {},
   "source": [
    "# Target calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba8887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_delay = F.when(F.col(\"ActualElapsedTime\")-F.col(\"CRSElapsedTime\") > 0, 1) \\\n",
    "                .otherwise(0).alias(\"total_delay\")\n",
    "    \n",
    "data = data.select('*', total_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e042d9",
   "metadata": {},
   "source": [
    "# Features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f5f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.select([c for c in data.columns if c != \"ActualElapsedTime\"])\n",
    "\n",
    "data = data.select(['Year', 'Month', 'DayOfWeek', \n",
    "                    'CRSDepTime', 'CRSArrTime', 'CRSElapsedTime', \n",
    "                    'Distance', 'total_delay'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0125a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.cache()\n",
    "data.write.mode('overwrite').format('noop').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94636c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5270893"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8372d035",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Month',\n",
       " 'DayOfWeek',\n",
       " 'CRSDepTime',\n",
       " 'CRSArrTime',\n",
       " 'CRSElapsedTime',\n",
       " 'Distance',\n",
       " 'total_delay']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac577d33",
   "metadata": {},
   "source": [
    "# Divide into train and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1e1afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Year: string, Month: string, DayOfWeek: string, CRSDepTime: string, CRSArrTime: string, CRSElapsedTime: string, Distance: string, total_delay: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed)\n",
    "train_data.write.mode('overwrite').format('noop').save()\n",
    "test_data.write.mode('overwrite').format('noop').save()\n",
    "\n",
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a5f9a",
   "metadata": {},
   "source": [
    "# AutoML params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ac8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {\n",
    "    \"target\": \"total_delay\"\n",
    "}\n",
    "task = SparkTask(\"binary\")\n",
    "use_algos = [[\"lgb\"]]\n",
    "cv = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715f2a8",
   "metadata": {},
   "source": [
    "# Fitting and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f8dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/LightAutoML/lightautoml/spark/ml_algo/boost_lgbm.py:398: RuntimeWarning: Maximum validation size for SparkBoostLGBM is exceeded: 2110227 > 10000. Reducing validation size down to maximum.\n",
      "  warnings.warn(f\"Maximum validation size for SparkBoostLGBM is exceeded: {valid_size} > {max_val_size}. \"\n",
      "[Stage 83:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 868841, number of negative: 1238372\n",
      "[LightGBM] [Warning] Only find one worker, will switch to serial tree learner\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0,105630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1705\n",
      "[LightGBM] [Info] Number of data points in the train set: 2107213, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,412318 -> initscore=-0,354393\n",
      "[LightGBM] [Info] Start training from score -0,354393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/projects/LightAutoML/lightautoml/spark/utils.py:191: RuntimeWarning: Attempting to calculate shape on not cached dataframe. It may take too much time.\n",
      "  warnings.warn(\"Attempting to calculate shape on not cached dataframe. \"\n",
      "/home/user/projects/LightAutoML/lightautoml/spark/ml_algo/boost_lgbm.py:398: RuntimeWarning: Maximum validation size for SparkBoostLGBM is exceeded: 2110227 > 10000. Reducing validation size down to maximum.\n",
      "  warnings.warn(f\"Maximum validation size for SparkBoostLGBM is exceeded: {valid_size} > {max_val_size}. \"\n",
      "[Stage 137:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 868841, number of negative: 1238372\n",
      "[LightGBM] [Warning] Only find one worker, will switch to serial tree learner\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0,042724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1039\n",
      "[LightGBM] [Info] Number of data points in the train set: 2107213, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0,412318 -> initscore=-0,354393\n",
      "[LightGBM] [Info] Start training from score -0,354393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "automl = SparkTabularAutoML(\n",
    "    spark=spark,\n",
    "    task=task,\n",
    "    general_params={\"use_algos\": use_algos},\n",
    "    lgb_params={'use_single_dataset_mode': True },\n",
    "    reader_params={\"cv\": cv, \"advanced_roles\": False}\n",
    ")\n",
    "\n",
    "oof_predictions = automl.fit_predict(\n",
    "    train_data,\n",
    "    roles=roles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d1921",
   "metadata": {},
   "source": [
    "# Score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07cbb28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF score: 0.648197628359205\n",
      "TEST score: 0.6606222253443365\n"
     ]
    }
   ],
   "source": [
    "score = task.get_dataset_metric()\n",
    "metric_value = score(oof_predictions)\n",
    "\n",
    "te_pred = automl.predict(test_data, add_reader_attrs=True)\n",
    "score = task.get_dataset_metric()\n",
    "test_metric_value = score(te_pred)\n",
    "\n",
    "print(f\"OOF score: {metric_value}\")\n",
    "print(f\"TEST score: {test_metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db589d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade1e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef509e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0131e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ec9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a0467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f877f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefd085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
