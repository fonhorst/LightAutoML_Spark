{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef6a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging.config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "from lightautoml.dataset.roles import CategoryRole\n",
    "from lightautoml.dataset.roles import DatetimeRole\n",
    "from lightautoml.dataset.roles import FoldsRole\n",
    "from lightautoml.dataset.roles import NumericRole\n",
    "from lightautoml.dataset.roles import TargetRole\n",
    "\n",
    "from lightautoml.spark.dataset.base import SparkDataset\n",
    "from lightautoml.spark.reader.base import SparkToSparkReader\n",
    "from lightautoml.spark.tasks.base import SparkTask\n",
    "from lightautoml.spark.pipelines.features.lgb_pipeline import SparkLGBSimpleFeatures\n",
    "\n",
    "from lightautoml.spark.transformers.categorical import SparkOrdinalEncoderTransformer\n",
    "from lightautoml.spark.transformers.datetime import SparkTimeToNumTransformer\n",
    "\n",
    "from lightautoml.spark.utils import (log_exec_timer, \n",
    "                                     logging_config, \n",
    "                                     VERBOSE_LOGGING_FORMAT, \n",
    "                                     SparkDataFrame)\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "from pyspark.ml import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e45dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.config.dictConfig(logging_config(level=logging.INFO, log_filename='/tmp/lama.log'))\n",
    "logging.basicConfig(level=logging.INFO, format=VERBOSE_LOGGING_FORMAT)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4ae40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session():\n",
    "    if os.environ.get(\"SCRIPT_ENV\", None) == \"cluster\":\n",
    "        return SparkSession.builder.getOrCreate()\n",
    "\n",
    "    spark_sess = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.jars\", \"../../jars/spark-lightautoml_2.12-0.1.jar\")\n",
    "        .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.5,org.apache.hadoop:hadoop-azure:3.3.2\")\n",
    "        .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "        .config(\"spark.driver.memory\", \"57g\")\n",
    "        .config(\"spark.executor.memory\", \"57g\")\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "\n",
    "    spark_sess.sparkContext.setCheckpointDir(\"/tmp/spark_checkpoints\")\n",
    "\n",
    "    spark_sess.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    return spark_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafbe022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 11:59:43 WARN Utils: Your hostname, desktop resolves to a loopback address: 127.0.1.1; using 192.168.0.104 instead (on interface wlp7s0)\n",
      "22/05/05 11:59:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/azamat/projects/LightAutoML/.venv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "https://mmlspark.azureedge.net/maven added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/azamat/projects/LightAutoML/.venv/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/azamat/.ivy2/cache\n",
      "The jars for the packages stored in: /home/azamat/.ivy2/jars\n",
      "com.microsoft.azure#synapseml_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-azure added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3ab76869-b833-4881-a899-638526db5a9b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#synapseml_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.azure#synapseml-core_2.12;0.9.5 in central\n",
      "\tfound org.scalactic#scalactic_2.12;3.0.5 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.4 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.2 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.6 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.10 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.10 in central\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.6 in central\n",
      "\tfound com.linkedin.isolation-forest#isolation-forest_3.2.0_2.12;2.0.8 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.testng#testng;6.8.8 in central\n",
      "\tfound org.beanshell#bsh;2.0b4 in central\n",
      "\tfound com.beust#jcommander;1.27 in central\n",
      "\tfound com.microsoft.azure#synapseml-deep-learning_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.azure#synapseml-opencv_2.12;0.9.5 in central\n",
      "\tfound org.openpnp#opencv;3.2.0-1 in central\n",
      "\tfound com.microsoft.cntk#cntk;2.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-cognitive_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.cognitiveservices.speech#client-jar-sdk;1.14.0 in central\n",
      "\tfound com.azure#azure-storage-blob;12.14.2 in central\n",
      "\tfound com.azure#azure-core;1.22.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.12.5 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.5 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;6.2.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound io.projectreactor#reactor-core;3.4.10 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound io.netty#netty-tcnative-boringssl-static;2.0.43.Final in central\n",
      "\tfound com.azure#azure-core-http-netty;1.11.2 in central\n",
      "\tfound io.netty#netty-handler;4.1.68.Final in central\n",
      "\tfound io.netty#netty-common;4.1.68.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.68.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.68.Final in central\n",
      "\tfound io.netty#netty-handler-proxy;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-socks;4.1.68.Final in user-list\n",
      "\tfound io.netty#netty-codec-http;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-epoll;4.1.68.Final in central\n",
      "\tfound io.netty#netty-transport-native-kqueue;4.1.68.Final in central\n",
      "\tfound io.projectreactor.netty#reactor-netty-http;1.0.11 in central\n",
      "\tfound io.netty#netty-resolver-dns;4.1.68.Final in central\n",
      "\tfound io.netty#netty-codec-dns;4.1.68.Final in central\n",
      "\tfound io.netty#netty-resolver-dns-native-macos;4.1.68.Final in central\n",
      "\tfound io.projectreactor.netty#reactor-netty-core;1.0.11 in central\n",
      "\tfound com.azure#azure-storage-common;12.14.1 in central\n",
      "\tfound com.azure#azure-storage-internal-avro;12.1.2 in central\n",
      "\tfound com.azure#azure-ai-textanalytics;5.1.4 in central\n",
      "\tfound com.microsoft.azure#synapseml-vw_2.12;0.9.5 in central\n",
      "\tfound com.github.vowpalwabbit#vw-jni;8.9.1 in central\n",
      "\tfound com.microsoft.azure#synapseml-lightgbm_2.12;0.9.5 in central\n",
      "\tfound com.microsoft.ml.lightgbm#lightgbmlib;3.2.110 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.0 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.2.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 1211ms :: artifacts dl 53ms\n",
      "\t:: modules in use:\n",
      "\tcom.azure#azure-ai-textanalytics;5.1.4 from central in [default]\n",
      "\tcom.azure#azure-core;1.22.0 from central in [default]\n",
      "\tcom.azure#azure-core-http-netty;1.11.2 from central in [default]\n",
      "\tcom.azure#azure-storage-blob;12.14.2 from central in [default]\n",
      "\tcom.azure#azure-storage-common;12.14.1 from central in [default]\n",
      "\tcom.azure#azure-storage-internal-avro;12.1.2 from central in [default]\n",
      "\tcom.beust#jcommander;1.27 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.0 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-xml;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.5 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;6.2.4 from central in [default]\n",
      "\tcom.github.vowpalwabbit#vw-jni;8.9.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.2.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from central in [default]\n",
      "\tcom.linkedin.isolation-forest#isolation-forest_3.2.0_2.12;2.0.8 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-cognitive_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-core_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-deep-learning_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-lightgbm_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-opencv_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml-vw_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.azure#synapseml_2.12;0.9.5 from central in [default]\n",
      "\tcom.microsoft.cntk#cntk;2.4 from central in [default]\n",
      "\tcom.microsoft.cognitiveservices.speech#client-jar-sdk;1.14.0 from central in [default]\n",
      "\tcom.microsoft.ml.lightgbm#lightgbmlib;3.2.110 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime_gpu;1.8.1 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-dns;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-codec-socks;4.1.68.Final from user-list in [default]\n",
      "\tio.netty#netty-common;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-handler-proxy;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver-dns;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-resolver-dns-native-macos;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-boringssl-static;2.0.43.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-epoll;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-kqueue;4.1.68.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.68.Final from central in [default]\n",
      "\tio.projectreactor#reactor-core;3.4.10 from central in [default]\n",
      "\tio.projectreactor.netty#reactor-netty-core;1.0.11 from central in [default]\n",
      "\tio.projectreactor.netty#reactor-netty-http;1.0.11 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.2 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.6 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
      "\torg.beanshell#bsh;2.0b4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.openpnp#opencv;3.2.0-1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.4 from central in [default]\n",
      "\torg.scalactic#scalactic_2.12;3.0.5 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.testng#testng;6.8.8 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.httpcomponents#httpclient;4.5.6 by [org.apache.httpcomponents#httpclient;4.5.13] in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.10 by [org.apache.httpcomponents#httpcore;4.4.13] in [default]\n",
      "\tcommons-codec#commons-codec;1.10 by [commons-codec#commons-codec;1.11] in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.5 by [com.fasterxml.jackson.core#jackson-core;2.13.0] in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 by [commons-logging#commons-logging;1.2] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;1.7.32] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   92  |   0   |   0   |   6   ||   86  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3ab76869-b833-4881-a899-638526db5a9b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 86 already retrieved (0kB/21ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 11:59:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851faaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "341943b2",
   "metadata": {},
   "source": [
    "# Loading and preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e3f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"../data/sampled_app_train.csv\",\n",
    "    usecols=[\n",
    "        \"TARGET\",\n",
    "        \"NAME_CONTRACT_TYPE\",\n",
    "        \"AMT_CREDIT\",\n",
    "        \"NAME_TYPE_SUITE\",\n",
    "        \"AMT_GOODS_PRICE\",\n",
    "        \"DAYS_BIRTH\",\n",
    "        \"DAYS_EMPLOYED\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc49c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"BIRTH_DATE\"] = (np.datetime64(\"2018-01-01\") + data[\"DAYS_BIRTH\"].astype(np.dtype(\"timedelta64[D]\"))).astype(str)\n",
    "data[\"EMP_DATE\"] = (\n",
    "    np.datetime64(\"2018-01-01\") + np.clip(data[\"DAYS_EMPLOYED\"], None, 0).astype(np.dtype(\"timedelta64[D]\"))\n",
    ").astype(str)\n",
    "data.drop([\"DAYS_BIRTH\", \"DAYS_EMPLOYED\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6392684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"__fold__\"] = np.random.randint(0, 5, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9914e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_sdf = spark.createDataFrame(data)\n",
    "dataset_sdf = dataset_sdf.select(\n",
    "    '*',\n",
    "    F.monotonically_increasing_id().alias(SparkDataset.ID_COLUMN)\n",
    ").cache()\n",
    "dataset_sdf.write.mode('overwrite').format('noop').save()\n",
    "dataset_sdf = dataset_sdf.select(F.col(\"__fold__\").cast(\"int\").alias(\"__fold__\"), *[c for c in dataset_sdf.columns if c != \"__fold__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4e4e625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- __fold__: integer (nullable = true)\n",
      " |-- TARGET: long (nullable = true)\n",
      " |-- NAME_CONTRACT_TYPE: string (nullable = true)\n",
      " |-- AMT_CREDIT: double (nullable = true)\n",
      " |-- AMT_GOODS_PRICE: double (nullable = true)\n",
      " |-- NAME_TYPE_SUITE: string (nullable = true)\n",
      " |-- BIRTH_DATE: string (nullable = true)\n",
      " |-- EMP_DATE: string (nullable = true)\n",
      " |-- _id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d17fc1",
   "metadata": {},
   "source": [
    "# Pipeline init and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707b2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = SparkTask(\"binary\")\n",
    "\n",
    "check_roles = {\n",
    "    TargetRole(): \"TARGET\",\n",
    "    CategoryRole(dtype=str): [\"NAME_CONTRACT_TYPE\", \"NAME_TYPE_SUITE\"],\n",
    "    NumericRole(np.float32): [\"AMT_CREDIT\", \"AMT_GOODS_PRICE\"],\n",
    "    DatetimeRole(seasonality=[\"y\", \"m\", \"wd\"]): [\"BIRTH_DATE\", \"EMP_DATE\"],\n",
    "    FoldsRole(): \"__fold__\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba137a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 12:00:25,495 INFO base base.py:225 Reader starting fit_read\n",
      "2022-05-05 12:00:25,500 INFO base base.py:226 \u001b[1mTrain data columns: ['__fold__', 'TARGET', 'NAME_CONTRACT_TYPE', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE', 'BIRTH_DATE', 'EMP_DATE', '_id']\u001b[0m\n",
      "\n",
      "2022-05-05 12:00:25,578 INFO utils utils.py:241 Cacher default_cacher (RDD Id: 14). Starting to materialize data.\n",
      "2022-05-05 12:00:25,972 INFO utils utils.py:243 Cacher default_cacher (RDD Id: 21). Finished data materialization.\n",
      "2022-05-05 12:00:28,420 INFO base base.py:375 Reader finished fit_read          \n"
     ]
    }
   ],
   "source": [
    "sreader = SparkToSparkReader(task=task, advanced_roles=False)\n",
    "sdataset = sreader.fit_read(dataset_sdf, roles=check_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62675615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features pipeline ctr\n"
     ]
    }
   ],
   "source": [
    "pipe = SparkLGBSimpleFeatures(cacher_key='cacher_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fa2609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 12:00:31,243 INFO base base.py:148 SparkFeaturePipeline is started\n",
      "2022-05-05 12:00:31,248 INFO base base.py:220 Number of layers in the current feature pipeline <lightautoml.spark.pipelines.features.lgb_pipeline.SparkLGBSimpleFeatures object at 0x7ff31c810490>: 1\n",
      "2022-05-05 12:00:31,250 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.base.SparkChangeRolesTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id']\n",
      "2022-05-05 12:00:31,252 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.datetime.SparkTimeToNumTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id']\n",
      "2022-05-05 12:00:31,322 INFO categorical categorical.py:222 [<class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderEstimator'> (ORD)] fit is started\n",
      "2022-05-05 12:00:32,101 INFO categorical categorical.py:243 [<class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderEstimator'> (ORD)] fit is finished\n",
      "2022-05-05 12:00:32,104 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.base.SparkChangeRolesTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id']\n",
      "2022-05-05 12:00:32,105 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.datetime.SparkTimeToNumTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id']\n",
      "2022-05-05 12:00:32,189 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id', 'dtdiff__BIRTH_DATE', 'dtdiff__EMP_DATE']\n",
      "2022-05-05 12:00:32,190 INFO categorical categorical.py:269 [<class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderTransformer'> (ORD)] transform is started\n",
      "2022-05-05 12:00:32,288 INFO categorical categorical.py:300 [<class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderTransformer'> (ORD)] Transform is finished\n",
      "2022-05-05 12:00:32,399 INFO utils utils.py:241 Cacher cacher_key (RDD Id: 78). Starting to materialize data.\n",
      "2022-05-05 12:00:32,580 INFO utils utils.py:243 Cacher cacher_key (RDD Id: 85). Finished data materialization.\n",
      "2022-05-05 12:00:32,583 INFO base base.py:163 SparkFeaturePipeline is finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparkDataset (DataFrame[_id: bigint, TARGET: int, __fold__: int, NAME_CONTRACT_TYPE: string, AMT_CREDIT: double, AMT_GOODS_PRICE: double, NAME_TYPE_SUITE: string, BIRTH_DATE: string, EMP_DATE: string, dtdiff__BIRTH_DATE: double, dtdiff__EMP_DATE: double, ord__NAME_CONTRACT_TYPE: double, ord__NAME_TYPE_SUITE: double])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.input_roles = sdataset.roles\n",
    "pipe.fit_transform(sdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53bb460",
   "metadata": {},
   "source": [
    "# Save pipeline transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0682bb34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 12:00:34,464 INFO mlwriters mlwriters.py:51 Save SparkChangeRolesTransformer to 'file:///tmp/SparkLGBSimpleFeatures/stages/0_PipelineModel_08f61fb776d0/stages/0_PipelineModel_72e080f5b198/stages/0_PipelineModel_ee518362475b/stages/0_SparkChangeRolesTransformer_e6edd42c2ab2'\n",
      "2022-05-05 12:00:35,706 INFO mlwriters mlwriters.py:51 Save SparkTimeToNumTransformer to 'file:///tmp/SparkLGBSimpleFeatures/stages/0_PipelineModel_08f61fb776d0/stages/0_PipelineModel_72e080f5b198/stages/0_PipelineModel_ee518362475b/stages/1_SparkTimeToNumTransformer_0235408d5ee1'\n",
      "2022-05-05 12:00:36,292 INFO mlwriters mlwriters.py:143 Save SparkOrdinalEncoderTransformer to 'file:///tmp/SparkLGBSimpleFeatures/stages/0_PipelineModel_08f61fb776d0/stages/0_PipelineModel_72e080f5b198/stages/0_PipelineModel_ee518362475b/stages/2_SparkOrdinalEncoderTransformer_2baa9d837965'\n"
     ]
    }
   ],
   "source": [
    "pipe.transformer.write().overwrite().save(\"file:///tmp/SparkLGBSimpleFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc1713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8554e4d",
   "metadata": {},
   "source": [
    "# Loading pipeline model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "056162c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azamat/projects/LightAutoML/.venv/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = PipelineModel.load(\"file:///tmp/SparkLGBSimpleFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fd132",
   "metadata": {},
   "source": [
    "# Expanding nested stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f50e15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "def expand_pipeline(pipeline_model):\n",
    "    for stage in pipeline_model.stages:\n",
    "        if type(stage) is PipelineModel:\n",
    "            expand_pipeline(stage)\n",
    "        else:\n",
    "            stages.append(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70708674",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_pipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca1b2af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparkChangeRolesTransformer_e6edd42c2ab2,\n",
       " SparkTimeToNumTransformer_0235408d5ee1,\n",
       " SparkOrdinalEncoderTransformer_2baa9d837965,\n",
       " DropColumnsTransformer_645632f801cd,\n",
       " NoOpTransformer_1295e8289d2e]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27e2574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95d753",
   "metadata": {},
   "source": [
    "# Attaching Debug Transformer after stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3085c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkDebugTransformer(Transformer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def _transform(self, dataset: SparkDataFrame) -> SparkDataFrame:\n",
    "        with log_exec_timer(f\"{self.uid}\"):\n",
    "            dataset = dataset.cache()\n",
    "            dataset.write.mode('overwrite').format('noop').save()\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b0d16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_debug_transformer(pipeline: PipelineModel, after_transformers: List):\n",
    "    stages = []\n",
    "    for stage in pipeline.stages:\n",
    "        stages.append(stage)\n",
    "        if type(stage) in after_transformers:\n",
    "            stages.append(SparkDebugTransformer())\n",
    "            \n",
    "    return PipelineModel(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724e0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attach_debug_transformer(model, [SparkOrdinalEncoderTransformer, SparkTimeToNumTransformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "622460ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparkChangeRolesTransformer_e6edd42c2ab2,\n",
       " SparkTimeToNumTransformer_0235408d5ee1,\n",
       " SparkDebugTransformer_6b62308ab2f5,\n",
       " SparkOrdinalEncoderTransformer_2baa9d837965,\n",
       " SparkDebugTransformer_4cb016c6be0f,\n",
       " DropColumnsTransformer_645632f801cd,\n",
       " NoOpTransformer_1295e8289d2e]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3fecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9adb0358",
   "metadata": {},
   "source": [
    "# Increasing dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86b7af06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11886bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 12:01:46 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated dataset size: 300000000\n"
     ]
    }
   ],
   "source": [
    "dataset_increase_factor = 30000\n",
    "execs = 1\n",
    "cores = 8\n",
    "\n",
    "if dataset_increase_factor > 1:\n",
    "    dataset_sdf = dataset_sdf.withColumn(\"new_col\", F.explode(F.array(*[F.lit(0) for i in range(dataset_increase_factor)])))\n",
    "    dataset_sdf = dataset_sdf.drop(\"new_col\")\n",
    "    dataset_sdf = dataset_sdf.select(\n",
    "        *[c for c in dataset_sdf.columns if c != SparkDataset.ID_COLUMN],\n",
    "        F.monotonically_increasing_id().alias(SparkDataset.ID_COLUMN),\n",
    "    ).cache()\n",
    "    dataset_sdf = dataset_sdf.repartition(execs * cores, SparkDataset.ID_COLUMN).cache()\n",
    "    dataset_sdf = dataset_sdf.cache()\n",
    "    dataset_sdf.write.mode('overwrite').format('noop').save()\n",
    "    print(f\"Duplicated dataset size: {dataset_sdf.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8fe5164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a669213",
   "metadata": {},
   "source": [
    "# Pipeline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b281dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 12:04:06,104 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.base.SparkChangeRolesTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id']\n",
      "2022-05-05 12:04:06,105 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.datetime.SparkTimeToNumTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id']\n",
      "2022-05-05 12:06:08,200 INFO utils utils.py:121 Exec time of SparkDebugTransformer_6b62308ab2f5 timer: 122.031888\n",
      "2022-05-05 12:06:08,202 INFO base base.py:169 In transformer <class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderTransformer'>. Columns: ['AMT_CREDIT', 'AMT_GOODS_PRICE', 'BIRTH_DATE', 'EMP_DATE', 'NAME_CONTRACT_TYPE', 'NAME_TYPE_SUITE', 'TARGET', '__fold__', '_id', 'dtdiff__BIRTH_DATE', 'dtdiff__EMP_DATE']\n",
      "2022-05-05 12:06:08,204 INFO categorical categorical.py:269 [<class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderTransformer'> (ORD)] transform is started\n",
      "2022-05-05 12:06:08,316 INFO categorical categorical.py:300 [<class 'lightautoml.spark.transformers.categorical.SparkOrdinalEncoderTransformer'> (ORD)] Transform is finished\n",
      "2022-05-05 12:08:18,182 INFO utils utils.py:121 Exec time of SparkDebugTransformer_4cb016c6be0f timer: 129.863718\n",
      "22/05/05 12:08:18 WARN CacheManager: Asked to cache already cached data.\n",
      "2022-05-05 12:08:26,822 INFO utils utils.py:121 Exec time of infer on dataset with 300000000 row: 260.719188\n"
     ]
    }
   ],
   "source": [
    "with log_exec_timer(f\"infer on dataset with {dataset_sdf.count()} row\"):\n",
    "    preds = model.transform(dataset_sdf)\n",
    "    preds = preds.cache()\n",
    "    preds.write.mode('overwrite').format('noop').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b786d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f00545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18810f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
